---
title: "Using `{sf}` in R"
output: 
  bookdown::html_document2:
    code_fold: show
    includes:
      in_header: assets/header.html
      after_body: assets/footer.html
---

```{r setup, include=FALSE, purl=FALSE, message=FALSE}

knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(icon)
library(here)

```

<style>
  .title{
    display: none;
  }
</style>

<br>
<br>

:::obj

 **Objectives for this section:**
 
 - make a spatial {`sf`} dataframe from X and Y columns (e.g., lat & lon)
 - introduce some of the many features of {`sf`} for spatial analysis
 - read and write spatial data using {`sf`} (`.shp`, `.gpx`, `.kml`, )
 
:::

<br>

## Spatial R: using `{sf}`

R is open source software, and more and more, it has become useful for analysis, visualization, and even writing. More recently, it has become a powerful tool for working with spatial data, making maps, etc. This is because it's free (open source), it can do nearly everything that a GUI type program can do (e.g., ArcGIS or QGIS), and you can write a script to do the same analysis many times over, saving time on repetitive tasks and making it clear how you did what you did. 

There are a variety of spatial mapping/plotting packages in R. However, the best option being used widely for vector-based spatial data in R is the [**`{sf}`**](http://r-spatial.github.io/sf/) package. **`{sf}`** is a powerful, clean, and fairly straightforward approach because it is fast, it can do most if not all of the tasks commonly done in other geospatial software programs. Even better, **`{sf}`** spatial objects are simply `data.frames` with a *sticky* geometry column, which makes it much easier to manipulate, tidy, join, and wrangle data. Basically all the geometry information gets compacted into a single column, and that column follows the data, no matter how you work with the data.

<br>


<center>

(ref:sfHorst) *Artwork by @allison_horst*

```{r sfIllustration, fig.cap='(ref:sfHorst)', echo=FALSE, out.width='90%'}

knitr::include_graphics("https://raw.githubusercontent.com/allisonhorst/stats-illustrations/master/rstats-artwork/sf.png")

```

</center>

<br>


### **Installing & Loading Spatial Packages**

The one caveat to using spatial packages in R is getting things installed and setup. Do this once, and you are good to go! But sometimes it can be a bit tricky. Ideally, we've all worked through this already, and so you have **`{sf}`** installed on your device currently. If you have already done this step, great! take a minute to look at the [**`{sf}`** webpage](http://r-spatial.github.io/sf/) and the various vignettes that are available (see [__Articles__](https://r-spatial.github.io/sf/articles/sf1.html) tab).

For our purposes, let's install and load these packages in R:

```{r installsf, echo=T, eval=F}

# install packages if you haven't already
install.packages(c("sf","mapview", "tmap", "USAboundaries"))

# load packages or "libraries"
library(tidyverse) # wrangling/plotting tools
library(viridis) # nice color palette
library(sf) # "simple features" spatial package for vector based data
library(mapview) # interactive web mapping
library(tmap) # static/interactive mapping
library(USAboundaries) # data for USA boundaries

```

```{r loadLibs, echo=F, warning=FALSE}
suppressPackageStartupMessages({
  library(tidyverse); # reading/writing files
  library(viridis); # nice color palette
  library(sf); # newer "simple features" spatial package
  library(mapview); # interactive web map
  library(tmap);
  library(USAboundaries)
  }) 

```

<br>

### **Load Data**

First we need to import some data. We'll be using the data we saved [from the previous lesson](m2_2_joins.html). We'll use the `data/csci_sites_match.rda` file, which since we are following the [data management/organization tips](02_project_management.html), we are in an RStudio Project, and we have a `data` folder with our data file inside! 

```{r load, echo=T, eval=T}

# Notice the relative path
load(file = "data/csci_sites_match.rda")

```
<br>

### **Inspect the Data**

If we look at a summary of the latitude and longitude, what do we notice? 

```{r summLats}

summary(csci_sites_match)

```

<br>

:::challenge

**Challenge**

For X/Y data (latitude, longitude, UTM Northing or Easting, etc), why might it be important to inspect these data before plotting?

:::


<details>
  <summary class="challenge-ans-title">**Click for Answers!**</summary>
  <div class="challenge-ans-body">
  
For data from the North American continent, and especially when using lat/long coordinates, keep an eye on **longitude**. It typically should be `negative`. A common snafu that can occur is a few values (or all) from the longitude column may remain positive, which means the data typically plots somewhere in the Indian Ocean. Make sure all your longitudes are negative (if in the US and using lat/longs). A similar issue that can arise may be a data entry error, where a UTM value is missing a digit, or a switch in digits occurs (i.e., 38.7 vs. 37.8). All more reason to check the range of your spatial data and make sure it makes sense!

Here's one way to fix or make sure you have all negative longitude (again, assuming we are working with data from N. America, and it's **WGS84** datum).

```{r Fixlatlong, echo=T, eval=F}

csci_sites_match$lon <- abs(csci_sites_match$lon) * -1 # make all values negative

range(csci_sites_match$lon)

```


  </div>
</details>

<br>

## Making Data Spatial

Once we have vetted our data, let's make it *spatial* and create a few quick test plots. To make an **`{sf}`** object, we are creating a `geometry` column. This is "sticky", and contains all the spatial information we need, and it lives within the dataframe. What's awesome is it can contain anything from information for a point to a line to a complex polygon. All in one column. *To make this, we need to know two important pieces...*

 - Which columns contain the spatial coordinates in (either name or column number)?
 - What [projection](http://spatialreference.org/ref/epsg/) or [EPSG (CRS, SRID, etc)](http://epsg.io/) is the data in/or that we want to use?

<br>

### **Datums, Projections, & CRS?**

One of the trickier things to figure out when working with spatial data is how it should be oriented or "projected" onto the earth's surface. Different parts of the world work in different Coordinate Reference Systems, or CRS. A really nice example of how this all works can be found via the [Data Carpentry Geospatial Lesson](https://datacarpentry.org/organization-geospatial/03-crs/). Here's the oversimplified way to think about it in relation to fruit!

Imagine an orange (or any ellipsoid fruit:  `r icon::fa("lemon", color="gold1")`). The &nbsp;`r icon::fa("globe-americas", color="steelblue")`&nbsp;isn't a perfect sphere! This is the **datum**, the underlying shape we use to represent our geographic space. Now, if we wanted to place a point on our fruit-shaped Earth, we could drape a piece of fabric over the ellipsoid shape and use it to draw our points or lines on. However, the fabric (or **projection**) isn't quite big enough to cover the whole globe, so near the edges things get messy, and you need to stretch things a bit to make it work. At the center of the fabric there's very little stretching and this is where things are most accurate. When working with spatial data, we typically want to find a **datum** and a **projection** (if applicable) that will give the least amount of stretching for the location/region we're working in. There are generally two kinds of CRS: *geographic*, and *projected*. 

 - **Geographic Coordinate Systems** use Latitude and Longitude and a **datum** to identify where the center of the fabric will be placed on our orange-shaped Earth.
 
 - **Projected Coordinate Systems** essentially convert the 3-dimensional orange into a 2-dimensional grid with X and Y points (Easting and Northing). Now we are taking just the orange peel of our globe, and trying to flatten it out. Projected Coordinate Systems are much more specific to a given region of the world because they must distort things to make the conversion work. 

There are CRS codes that we can use to represent all these things, and {`sf`} makes this easy to integrate into our data.

A few common CRS Projections used in California for state/federal work:

 - **Projected**: [NAD83 California Albers: EPSG 3310](http://epsg.io/3310)
 - **Projected**: [NAD83 California Teal Albers: SR-ORG:10](http://spatialreference.org/ref/sr-org/10/)
 - **Geographic**: [WGS84 Lat Lon: EPSG 4326](http://epsg.io/4326)

<br>

<details><summary class="extra-practice-title">Extra Info!</summary>
  <div class="extra-practice-body">
  This is a complicated subject and don't expect to learn it all at once. For more details, we definitely recommend reading through Robin Lovelace's Geocomputation in R book, especially the sections on CRS and Reprojecting your data.

  - [Section 2.4 on Coordinate Reference Systems (CRS)](https://geocompr.robinlovelace.net/spatial-class.html#crs-intro)
  - [Section 6.1 on Reprojecting Data](https://geocompr.robinlovelace.net/reproj-geo-data.html)
  </div>
</details>
<br>

<br>

### **Dataframe to `{sf}` dataframe**

To make a dataframe with X and Y coordinates into an **`{sf}`** dataframe (so it's spatial and we can do all kinds of cool spatial/mapping operations), we can use the `st_as_sf()` function. We will also add a CRS projection so the data will show up in the right part of the world.

```{r makespatial, eval=TRUE, echo=TRUE}

# make data sf object: 
df_sf <- st_as_sf(csci_sites_match,
                  coords = c("lon", "lat"), # note we put lon (or X) first!
                  #coords = c(10, 9), # can use numbers here too
                  remove = F, # don't remove these lat/lon cols from the dataframe
                  crs = 4326) # add projection (this is WGS84)

```

<br>

### **Transform to different projection?**

Now that our data is in this format, it's pretty easy to transform/convert to another spatial projection. Here we can check our current CRS, then switch the projection over to something different.

```{r transformSF, eval=T, echo=T}

# check CRS first:
st_crs(df_sf)

# change CRS using st_transform
df_sf_albers <- st_transform(df_sf, crs=3310)

# check that it changed
st_crs(df_sf_albers)

```

<br>


## Plot `{sf}` Data

Now we can make a few quick plots to see how our data looks. The simplest option is to use the `plot` function. However, if we try to plot by running: 

 - `plot(df_sf)`
 
 We see something like this:
 
 ![](images/plot_df_sf.png)

Base-plotting functions work with **`{sf}`**, but be aware, using `plot` will default to plotting a facet or map for every **column** of data in your dataframe. Avoid that by specifying `st_coordinates()` or `$geometry` to ensure only the spatial data gets plotted.

### **Plotting with `plot`**

```{r singleSF1, echo=TRUE}

# single layer
plot(df_sf$geometry)

# add some flair but still a single layer
plot(df_sf$geometry, col = "orange")

```

<br>

:::challenge

 **Challenge:**
 
 - **How can we find out how to change the shape of the points using the `plot` function?**
 
:::

<br>

<details>
  <summary class="challenge-ans-title">**Click for Answers!**</summary>
  <div class="challenge-ans-body">

**Ultimately we can make our plots as fancy as we want. But to figure the above question out, I'd recommend these options:**

 - Google: *"[R] how to change shape of points using plot"*
 - Use built-in R help: `?plot` or `?points`
 - Which may help you find **`pch`**...try `?pch` and scroll to the bottom.
 </div>
</details>

<br>

### **Fancy Plot**

Let's add a few additional features to our map, using the baseplotting `plot()` function, and adding an `ifelse` statement to plot different colors for CSCI scores above or below 0.75.

```{r singleSF2, eval=T, echo=T}

# this is better
plot(df_sf$geometry, 
     pch=16, 
     # purple dots are CSCI > 0.75, yellow are <0.75
     col=ifelse(df_sf$CSCI>0.75, adjustcolor("purple4", alpha=0.7), "gold"), cex=1.5, xlab = "Longitude", ylab="Latitude")
graphics::title("CSCI ( >0.75=purple, <0.75=yellow)")

```

Ok, so now we can make our data spatial, we can look at some preliminary plots, but what's next? 

<br><br>

## Spatial Operations

What about other spatial data or operations? The `sf` package can do nearly all the same things you can do in GIS software, like **buffer**, **crop**, **clip**, **merge**, etc. For a full list and some nice vignettes, check out the `sf` page: https://r-spatial.github.io/sf/, and click on the **Articles** tab.

There's simply too much to show and not enough time, but below are a few options including:

 - how to crop or clip one spatial dataset using another spatial dataset
 - how to create a buffer
 - how to read/write `shapefiles`.

### **Get State & County Boundary Data**

Let's use some boundaries to crop/buffer and manipulate our data. There are loads of options available, let's look at a package called `USAboundaries`, which allows us to quickly download county or state outlines, in `sf` formats. Very handy for making some quick professional maps. Let's get an outline of CA, California counties, select and buffer some points.

```{r getCA, echo=T, eval=T}

library(USAboundaries)

# Pick a State
state_names <- c("california") # notice we can add more states to this list if we want

# Download STATE data and add projection
CA <- us_states(resolution = "high", states = state_names) # what does resolution do?

st_crs(CA) # double check the CRS

# make a quick plot!
plot(CA$geometry)

# add data from above? use "add=TRUE"
plot(df_sf$geometry, col="blue", add=TRUE)

```

<br>

Next let's create a California county object. First we download all California counties, then we filter to just El Dorado County.

```{r getCAcounties, echo=T, eval=T}

# Download outlines of all the counties for California
CA_counties<-us_counties(resolution = "high", states = "CA") 

# Pick a county of interest
co_names <- c("El Dorado") # notice we can add more states to this list if we want

# filter to just that county:
eldor_co <- filter(CA_counties, name==co_names)


```

<br>

### **Crop points to County**

Next we want to clip our point data down to only points in El Dorado County. The quickest way to do this is using `st_intersection()`.

```{r stIntersection}

# we list the thing we want to crop first, then what we crop by second
eldor_pts <- st_intersection(df_sf, eldor_co)

plot(eldor_co$geometry)
plot(df_sf$geometry, add=T, col="gray") # all the points
plot(eldor_pts$geometry, add=T, bg ="purple", pch=21) # just the points we cropped

```

### **Add Some River & Watershed Data**

Here we can use the **[{`nhdplusTools`}](https://usgs-r.github.io/nhdplusTools/)** package to grab river lines and watershed basin boundaries to add to our map.

```{r}

library(nhdplusTools)

# first identify the gage of interest: This on on the American
nldi_nwis <- list(featureSource = "nwissite", featureID = "USGS-11444500")

# next download the basin:
basin <- get_nldi_basin(nldi_feature = nldi_nwis)

# find out comid of the gage or point:
comid_pt <- discover_nhdplus_id(nldi_feature = nldi_nwis)

# use the comid to download streamlines U/S or D/S of the gage
# here downstream with diversions for 10 km
us_streamlines <- navigate_nldi(nldi_feature = nldi_nwis,
                                mode="upstreamTributaries",
                                distance_km =  100)

us_main <- navigate_nldi(nldi_feature = nldi_nwis,
                                mode="UM",
                                distance_km =  100)

# find other upstream gages or NWIS sites
us_gages <- navigate_nldi(nldi_feature = nldi_nwis, 
                          mode="UM",
                          data_source = "nwissite",
                          distance=100)

#plot and add to previous plot
plot(eldor_co$geometry, border="black", lty=2, lwd=1.5)
plot(us_streamlines$geometry, col="steelblue", add=T)
plot(us_main$geometry, col="darkblue", add=T, lwd=2)
plot(df_sf$geometry, add=T, pch=23, col="gray", bg=alpha("orange", 0.4)) # all the points
plot(eldor_pts$geometry, add=T, bg ="purple2", pch=21) # just in county
plot(us_gages$geometry, bg="maroon", pch=21, add=T)
plot(basin$geometry, add=T, border = alpha("darkblue", alpha = 0.5), lwd=2)

```


### **Buffer Points**

Let's add one additional tool into the mix. `st_buffer` which allows us to buffer our data. Since our data is in lat/lon, the units of distance are in *degrees*, which can make it hard to judge. First, let's transform our data to a projected UTM (so the units are in meters instead of degrees). Then let's add a 100 meter buffer around our CSCI points.

```{r buffer, echo=T, eval=T}

basin_UTM <- st_transform(basin, crs=3310)
basin_buff_1km <- st_buffer(basin_UTM, dist = 1000)


plot(st_transform(eldor_co$geometry, 3310))
plot(basin_buff_1km$geometry, bg="blue", col="gray", pch=21, add=T)
plot(basin_UTM$geometry, col=alpha("seagreen", 0.5), border="forestgreen", pch=21, lwd=1.9, add=T)

```

 

<br>

Let's look at the next lesson to find additional ways to make maps that we could actually use or share.


Great work! Let's [move to the next lesson](m2_4_mapping_w_sf.html).